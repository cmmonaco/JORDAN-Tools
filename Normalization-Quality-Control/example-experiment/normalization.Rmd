---
title: "Normalization Report for Example Experiemnt"
date: "Dec. 23, 2018"
authhor: "C.M. Monaco"
output:
  html_notebook:
    code_folding: hide
---

This is a self-documenting template which performs the standard normalization process used by the Dahlman Lab for raw barcode counts.

This is being run on real experimental data from the Dahlman Lab as an example of the Normalization and Post-Normalization Quality Control process.

```{r}
## EDIT##
# Read in raw counts data from CSV
counts <- data.frame(read.table("raw_counts.csv", #<- set the path to the raw counts file here
                                header = TRUE,
                                sep = ",",
                                row.names = 1))

# Capture Inputs (the string "input" must appear somewhere in the sample name to be identified)
inputs <- counts[, grep(".*input*", colnames(counts), ignore.case = TRUE)]

# Rename Column Names to letters A,B,C, etc.
colnames(inputs) <- LETTERS[1:length(inputs)]

# Remove inputs from counts variable
counts[, grep(".*input*", colnames(counts), ignore.case = TRUE)] <- NULL

# Include required Libraries
library(ggplot2)
```

# Input Analysis and Normalization

The inputs are normalized to unity and then a pairwise Pearson Correlation test is conducted to ensure all inputs show high correlation. Those replicates failing the test should manually be removed from averaging before the next step.

```{r, fig.width = 9}
# Normalize all inputs
norm_inputs <- data.frame(matrix(ncol = ncol(inputs), nrow = nrow(inputs)),
                          row.names = rownames(inputs))
colnames(norm_inputs) <- colnames(inputs)

for(n in colnames(inputs))
{
  norm_inputs[n] <- inputs[n] / sum(inputs[n])
}

# Input Replicate Correlation Tests
panel.cor <- function(x, y, digits = 2, cex.cor, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  # correlation coefficient
  r <- cor(x, y, method = "pearson")
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste("r= ", txt, sep = "")
  text(0.5, 0.6, txt)
  # p-value calculation
  p <- cor.test(x, y)$p.value
  txt2 <- format(c(p, 0.123456789), digits = digits)[1]
  txt2 <- paste("p= ", txt2, sep = "")
  if(p<0.01)
  {
    txt2 <- paste("p= ", "<0.01", sep = "")
    text(0.5, 0.4, txt2)
  }
  else
  {
    text(0.5, 0.4, txt2, col = "red")
  }
}
pairs(norm_inputs, upper.panel = panel.cor)
```

All input replicates show high correlation.

## Average input barcode distribution.

Input replicates passing the Pearson Correlation test are then averaged together and the count distribution of each barcode is plotted.

```{r, fig.width=10}
## EDIT ##
# Remove any replicates not passing correlation test.
#norm_inputs$A <- NULL

# Average input replicates
norm_input_avg <- data.frame(rowSums(norm_inputs) / ncol(norm_inputs),
                             row.names = rownames(inputs))
colnames(norm_input_avg) <- "avg"

ggplot(data = norm_input_avg,aes(x = reorder(rownames(norm_input_avg), -avg), y = avg)) +
  geom_bar(stat = "identity", color = "blue4", fill = "white") + 
  theme(axis.text.x = element_text(angle = 90, size = 6)) +
  labs(title = "Normalized Input Barcode Distribution" ,x = "Sample Names", y = "Normalized Counts")
```

## Count Threshold Determination

A threshold is now established to determined which barcodes are not present in high enough numbers to represent significant counts. This is a qualitative process in which the average of all counts are computed and a threshold is determined as a multiple below the average.

The average is computed and candidate thresholds are shown overlayed on the bar plot.

```{r, fig.width = 10}
#Only average values that are non-zero as to not skew mean
norm_mean_input <- mean(norm_input_avg[norm_input_avg != 0,1])

# Generate bar plot showing barcode distribution
ggplot(data = norm_input_avg,aes(x = reorder(rownames(norm_input_avg), -avg), y = avg)) +
  geom_bar(stat = "identity", color = "blue4", fill = "white") + 
  theme(axis.text.x = element_text(angle = 90, size = 6), legend.position = c(0.85,0.9)) +
  labs(title = "Normalized Input Barcode Distribution" ,x = "Sample Names", y = "Normalized Counts") +
  geom_hline(aes(yintercept = norm_mean_input/5, linetype = "5x"), color = "red", show.legend = TRUE) + 
  geom_hline(aes(yintercept = norm_mean_input/10, linetype = "10x"), color = "green", show.legend = TRUE) +
  scale_linetype_manual(name = "Threshold", values = c(1,1), guide = guide_legend(override.aes = list(color = c("green", "red"))))
```

We want to choose a threshold at which we remove the lowest count barcodes while retaining as many barcodes as possible. This is typically 10x below the average.

```{r}
# Create list of BCs under threshold
names_under_thresh <- rownames(subset(norm_input_avg, norm_input_avg < (norm_mean_input/10)))
cat("List of Removed Barcodes\n", names_under_thresh)

# Remove these barcodes from our counts list 
counts <- counts[! rownames(counts) %in% names_under_thresh, ]
inputs <- inputs[! rownames(inputs) %in% names_under_thresh, ]
```

# Sample Normalization

## Input Re-normailzation

Inputs must be re-normalized and re-averaged to account for the barcodes that have been removed.

```{r}
#Re-normalize all inputs
norm_inputs <- data.frame(matrix(ncol = ncol(inputs), nrow = nrow(inputs)),
                          row.names = rownames(inputs))
colnames(norm_inputs) <- colnames(inputs)

for(n in colnames(inputs))
{
  norm_inputs[n] <- inputs[n] / sum(inputs[n])
}

# Average input replicates
norm_input_avg <- data.frame(rowSums(norm_inputs) / ncol(norm_inputs),
                             row.names = rownames(inputs))
colnames(norm_input_avg) <- "avg"
```

## Sample Normalization

Finally, the samples are normalized back to the input and then scaled so that each sample's counts total 100. 

```{r}
#First, remove samples whose columns sum to zero.
counts_nz <- counts[colSums(counts) != 0]
#List samples removed
if(length(names(counts[colSums(counts) == 0])) != 0) {
  cat("Samples removed for zero counts\n", names(counts[colSums(counts) == 0]))
} else {
  cat("All Samples Retained")
}
```


```{r}
#Normalize each sample to unity
selfnorm_counts <- sweep(counts_nz, 2, colSums(counts_nz), '/')

#Normalize counts to normalized input average
inputnorm_counts <- selfnorm_counts / as.matrix(norm_input_avg)

#Scale counts so each column totals 100
scale_factor <- 100 / colSums(inputnorm_counts)
final_norm_counts <- as.matrix(inputnorm_counts) %*% diag(scale_factor)
dimnames(final_norm_counts) <- list(rownames(inputnorm_counts), colnames(inputnorm_counts))

#Write final normalized to counts to output CSV file
write.csv(file = "norm_counts_output/norm_counts.csv", final_norm_counts) 
```

Normalized counts written to the file `norm_counts_output/norm_counts.csv`.